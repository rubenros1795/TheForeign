{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "import string\n",
    "import numpy\n",
    "import re\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname, encoding = 'utf-8') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "all_txt = glob.glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period is small enough\n",
      "22988 lines opened from: 1815_cleaned.txt\n",
      "22024 lines opened from: 1816_cleaned.txt\n",
      "23146 lines opened from: 1817_cleaned.txt\n",
      "24486 lines opened from: 1818_cleaned.txt\n",
      "20386 lines opened from: 1819_cleaned.txt\n",
      "23180 lines opened from: 1820_cleaned.txt\n",
      "23722 lines opened from: 1821_cleaned.txt\n",
      "22950 lines opened from: 1822_cleaned.txt\n",
      "23532 lines opened from: 1823_cleaned.txt\n",
      "24242 lines opened from: 1824_cleaned.txt\n",
      "period written to disk, on to the next\n",
      "period is small enough\n",
      "23510 lines opened from: 1825_cleaned.txt\n",
      "24066 lines opened from: 1826_cleaned.txt\n",
      "22404 lines opened from: 1827_cleaned.txt\n",
      "23650 lines opened from: 1828_cleaned.txt\n",
      "25424 lines opened from: 1829_cleaned.txt\n",
      "28104 lines opened from: 1830_cleaned.txt\n",
      "31080 lines opened from: 1831_cleaned.txt\n",
      "35238 lines opened from: 1832_cleaned.txt\n",
      "33646 lines opened from: 1833_cleaned.txt\n",
      "37462 lines opened from: 1834_cleaned.txt\n",
      "period written to disk, on to the next\n",
      "an average of 349274 lines a file is going to be imported in period 1835\n",
      "period written to disk, on to the next\n",
      "an average of 426798 lines a file is going to be imported in period 1845\n",
      "period written to disk, on to the next\n",
      "an average of 461053 lines a file is going to be imported in period 1855\n",
      "period written to disk, on to the next\n",
      "an average of 413357 lines a file is going to be imported in period 1865\n",
      "period written to disk, on to the next\n",
      "an average of 441016 lines a file is going to be imported in period 1875\n",
      "period written to disk, on to the next\n",
      "an average of 426403 lines a file is going to be imported in period 1885\n",
      "period written to disk, on to the next\n",
      "an average of 427667 lines a file is going to be imported in period 1895\n",
      "period written to disk, on to the next\n",
      "an average of 505962 lines a file is going to be imported in period 1905\n",
      "period written to disk, on to the next\n"
     ]
    }
   ],
   "source": [
    "for start_year in list(range(1815,1915,10)):\n",
    "    list_txt_range = [t for t in all_txt if int(t[0:4]) >= start_year and int(t[0:4]) < start_year + 10]\n",
    "    \n",
    "    # Get Size of Range\n",
    "    range_size = 0\n",
    "    numb_lines = 0\n",
    "    \n",
    "    for txt in list_txt_range:\n",
    "        statinfo = os.stat(txt)\n",
    "        range_size += int(statinfo.st_size)\n",
    "        numb_lines += file_len(txt)\n",
    "    \n",
    "    range_size = round(range_size/ 1000000)\n",
    "        \n",
    "    sample_size = 801\n",
    "    \n",
    "    if range_size > 800:\n",
    "        tot_lines_samp = round(sample_size * numb_lines / range_size)\n",
    "        print('an average of ' + str(tot_lines_samp) + ' lines a file is going to be imported in period ' + str(start_year))\n",
    "        \n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.read().splitlines()\n",
    "                \n",
    "                if len(content) <= round(tot_lines_samp / 10):\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "                \n",
    "                if len(content) > round(tot_lines_samp / 10):\n",
    "                    content = random.sample(content, round(tot_lines_samp / 10))\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "        \n",
    "        with open('period_sample_' + str(start_year) + '.txt', 'w', encoding = 'utf-8') as file:\n",
    "            for item in lines_period:\n",
    "                    file.write(\"%s\\n\" % item)\n",
    "                    \n",
    "        print('period written to disk, on to the next')\n",
    "    \n",
    "    else:\n",
    "        print('period is small enough')\n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.readlines()\n",
    "                #print(str(len(content)) + ' lines opened from: ' + txt)\n",
    "                for line in content:\n",
    "                        lines_period.append(line)\n",
    "        \n",
    "        with open('period_sample_' + str(start_year) + '.txt', 'w', encoding = 'utf-8') as file:\n",
    "            for item in lines_period:\n",
    "                    file.write(\"%s\\n\" % item)\n",
    "        print('period written to disk, on to the next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Txt_Lines(txt):\n",
    "    with open(txt, encoding = 'utf-8') as f:\n",
    "            content = f.read().split('\\n')\n",
    "    \n",
    "    lines = list()\n",
    "    \n",
    "    for line in content:\n",
    "        tmp = line.split(' ')\n",
    "        lines.append(tmp)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for txt in list_sample_txt:\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "        sentences = Txt_Lines(txt)\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=100, workers=6)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(txt[14:18])+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "\n",
    "    if count > 0:\n",
    "        sentences = Txt_Lines(txt)\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(txt[14:18])+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period is small enough\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n",
      "period is small enough\n",
      "Model saved, on to the next\n",
      "\n",
      "period is small enough\n",
      "Model saved, on to the next\n",
      "\n",
      "an average of 568828 lines a file is going to be imported in period 1845\n",
      "Model saved, on to the next\n",
      "\n",
      "an average of 614481 lines a file is going to be imported in period 1855\n",
      "Model saved, on to the next\n",
      "\n",
      "an average of 550914 lines a file is going to be imported in period 1865\n",
      "Model saved, on to the next\n",
      "\n",
      "an average of 587777 lines a file is going to be imported in period 1875\n",
      "Model saved, on to the next\n",
      "\n",
      "an average of 568301 lines a file is going to be imported in period 1885\n",
      "Model saved, on to the next\n",
      "\n",
      "an average of 569986 lines a file is going to be imported in period 1895\n",
      "Model saved, on to the next\n",
      "\n",
      "an average of 674335 lines a file is going to be imported in period 1905\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Combined\n",
    "count = 0\n",
    "\n",
    "for start_year in list(range(1815,1915,10)):\n",
    "    list_txt_range = [t for t in all_txt if int(t[0:4]) >= start_year and int(t[0:4]) < start_year + 10]\n",
    "    \n",
    "    # Get Size of Range\n",
    "    range_size = 0\n",
    "    numb_lines = 0\n",
    "    \n",
    "    for txt in list_txt_range:\n",
    "        statinfo = os.stat(txt)\n",
    "        range_size += int(statinfo.st_size)\n",
    "        numb_lines += file_len(txt)\n",
    "    \n",
    "    range_size = round(range_size/ 1000000)\n",
    "        \n",
    "    sample_size = 801\n",
    "    \n",
    "    if range_size > 800:\n",
    "        tot_lines_samp = round(sample_size * numb_lines / range_size)\n",
    "        print('an average of ' + str(tot_lines_samp) + ' lines a file is going to be imported in period ' + str(start_year))\n",
    "        \n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.read().splitlines()\n",
    "                \n",
    "                if len(content) <= round(tot_lines_samp / 10):\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "                \n",
    "                if len(content) > round(tot_lines_samp / 10):\n",
    "                    content = random.sample(content, round(tot_lines_samp / 10))\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "            \n",
    "    else:\n",
    "        print('period is small enough')\n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.readlines()\n",
    "                for line in content:\n",
    "                        lines_period.append(line)\n",
    "        \n",
    "    if count == 0:\n",
    "        count += 1\n",
    "        sentences = [l.split(' ') for l in lines_period]\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=100, workers=6)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(start_year)+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "\n",
    "    if count > 0:\n",
    "        sentences = [l.split(' ') for l in lines_period]\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(start_year)+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")      \n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getw(word, n):\n",
    "    tmp = model.most_similar(word)[n][0]\n",
    "    tmp = \" \" + tmp + \" \"\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5114883\n",
      "0.53946126\n",
      "0.5609166\n",
      "0.5714821\n",
      "0.5344653\n",
      "0.5383246\n",
      "0.565197\n",
      "0.61158526\n",
      "0.57237357\n",
      "0.59126824\n"
     ]
    }
   ],
   "source": [
    "list_mod = glob.glob('*.w2v')\n",
    "\n",
    "for i in list(range(1815,1906,10)):\n",
    "    mfn = [m for m in list_mod if int(m[0:4]) == i]\n",
    "    model = Word2Vec.load(mfn[0])\n",
    "    word = 'buitenlandsche'\n",
    "    #print(str(i) + ' ' + getw(word,0) + getw(word,1) + getw(word,2) + getw(word,3))\n",
    "    print(str(model.similarity('moederland', 'vaderland')))# + \" | \" + str(model.similarity('binnenlandsche', 'vaderlandsche'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'duitschland'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getw('buitenland',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans;\n",
    "from sklearn.neighbors import KDTree;\n",
    "Z = model.wv.syn0\n",
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    return kmeans_clustering.cluster_centers_, idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, clusters = clustering_on_wordvecs(Z, 100);\n",
    "centroid_map = dict(zip(model.wv.index2word, clusters));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt;\n",
    "from itertools import cycle;\n",
    "\n",
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "#Closest points for each Cluster center is used to query the closest 20 points to it.\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "#Word Index is queried for each position in the above array, and added to a Dictionary.\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "#A DataFrame is generated from the dictionary.\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "top_words = get_top_words(model.wv.index2word, 5000, centers, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D://Scriptie//Data//models\")\n",
    "list_mod = glob.glob('*.w2v')\n",
    "\n",
    "for i in list(range(1815,1906,10)):\n",
    "    mfn = [m for m in list_mod if int(m[0:4]) == i]\n",
    "    model = Word2Vec.load(mfn[0])\n",
    "    print(str(i) + \": \" + \" | \".join(list(dict(model.most_similar(\"vreemdeling\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "import string\n",
    "import numpy\n",
    "import re\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "stop = [word.split() for word in open(\"C:\\\\Users\\\\Ruben\\\\Documents\\\\Artikelen\\\\Joris\\\\stopwords-nl.txt\", 'r', encoding = \"utf-8\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname, encoding = 'utf-8') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "all_txt = glob.glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period is small enough\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f44ece054e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'period_sample_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_year\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines_period\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'period written to disk, on to the next'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for start_year in list(range(1815,1915,10)):\n",
    "    list_txt_range = [t for t in all_txt if int(t[0:4]) >= start_year and int(t[0:4]) < start_year + 10]\n",
    "    \n",
    "    # Get Size of Range\n",
    "    range_size = 0\n",
    "    numb_lines = 0\n",
    "    \n",
    "    for txt in list_txt_range:\n",
    "        statinfo = os.stat(txt)\n",
    "        range_size += int(statinfo.st_size)\n",
    "        numb_lines += file_len(txt)\n",
    "    \n",
    "    range_size = round(range_size/ 1000000)\n",
    "        \n",
    "    sample_size = 801\n",
    "    \n",
    "    if range_size > 800:\n",
    "        tot_lines_samp = round(sample_size * numb_lines / range_size)\n",
    "        print('an average of ' + str(tot_lines_samp) + ' lines a file is going to be imported in period ' + str(start_year))\n",
    "        \n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.read().splitlines()\n",
    "                \n",
    "                if len(content) <= round(tot_lines_samp / 10):\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "                \n",
    "                if len(content) > round(tot_lines_samp / 10):\n",
    "                    content = random.sample(content, round(tot_lines_samp / 10))\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "        \n",
    "        with open('period_sample_' + str(start_year) + '.txt', 'w', encoding = 'utf-8') as file:\n",
    "            for item in lines_period:\n",
    "                    file.write(\"%s\\n\" % item)\n",
    "                    \n",
    "        print('period written to disk, on to the next')\n",
    "    \n",
    "    else:\n",
    "        print('period is small enough')\n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.readlines()\n",
    "                #print(str(len(content)) + ' lines opened from: ' + txt)\n",
    "                for line in content:\n",
    "                        lines_period.append(line)\n",
    "        \n",
    "        with open('period_sample_' + str(start_year) + '.txt', 'w', encoding = 'utf-8') as file:\n",
    "            for item in lines_period:\n",
    "                    file.write(\"%s\\n\" % item)\n",
    "        print('period written to disk, on to the next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Txt_Lines(txt):\n",
    "    with open(txt, encoding = 'utf-8') as f:\n",
    "            content = f.read().split('\\n')\n",
    "    \n",
    "    lines = list()\n",
    "    \n",
    "    for line in content:\n",
    "        tmp = line.split(' ')\n",
    "        lines.append(tmp)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for txt in list_sample_txt:\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "        sentences = Txt_Lines(txt)\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=100, workers=6)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(txt[14:18])+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "\n",
    "    if count > 0:\n",
    "        sentences = Txt_Lines(txt)\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(txt[14:18])+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined\n",
    "count = 0\n",
    "\n",
    "for start_year in list(range(1815,1915,10)):\n",
    "    list_txt_range = [t for t in all_txt if int(t[0:4]) >= start_year and int(t[0:4]) < start_year + 10]\n",
    "    \n",
    "    # Get Size of Range\n",
    "    range_size = 0\n",
    "    numb_lines = 0\n",
    "    \n",
    "    for txt in list_txt_range:\n",
    "        statinfo = os.stat(txt)\n",
    "        range_size += int(statinfo.st_size)\n",
    "        numb_lines += file_len(txt)\n",
    "    \n",
    "    range_size = round(range_size/ 1000000)\n",
    "        \n",
    "    sample_size = 801\n",
    "    \n",
    "    if range_size > 800:\n",
    "        tot_lines_samp = round(sample_size * numb_lines / range_size)\n",
    "        print('an average of ' + str(tot_lines_samp) + ' lines a file is going to be imported in period ' + str(start_year))\n",
    "        \n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.read().splitlines()\n",
    "                \n",
    "                if len(content) <= round(tot_lines_samp / 10):\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "                \n",
    "                if len(content) > round(tot_lines_samp / 10):\n",
    "                    content = random.sample(content, round(tot_lines_samp / 10))\n",
    "                    for line in content:\n",
    "                        lines_period.append(line)\n",
    "            \n",
    "    else:\n",
    "        print('period is small enough')\n",
    "        lines_period = list()\n",
    "        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.readlines()\n",
    "                for line in content:\n",
    "                        lines_period.append(line)\n",
    "        \n",
    "    if count == 0:\n",
    "        count += 1\n",
    "        sentences = [l.split(' ') for l in lines_period]\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=100, workers=6, window = 200, iter=10)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(start_year)+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "\n",
    "    if count > 0:\n",
    "        sentences = [l.split(' ') for l in lines_period]\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(start_year)+\"_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")      \n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getw(word, n):\n",
    "    tmp = model.most_similar(word)[n][0]\n",
    "    tmp = \" \" + tmp + \" \"\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5114883\n",
      "0.53946126\n",
      "0.5609166\n",
      "0.5714821\n",
      "0.5344653\n",
      "0.5383246\n",
      "0.565197\n",
      "0.61158526\n",
      "0.57237357\n",
      "0.59126824\n"
     ]
    }
   ],
   "source": [
    "list_mod = glob.glob('*.w2v')\n",
    "\n",
    "for i in list(range(1815,1906,10)):\n",
    "    mfn = [m for m in list_mod if int(m[0:4]) == i]\n",
    "    model = Word2Vec.load(mfn[0])\n",
    "    word = 'buitenlandsche'\n",
    "    #print(str(i) + ' ' + getw(word,0) + getw(word,1) + getw(word,2) + getw(word,3))\n",
    "    print(str(model.similarity('moederland', 'vaderland')))# + \" | \" + str(model.similarity('binnenlandsche', 'vaderlandsche'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'duitschland'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getw('buitenland',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans;\n",
    "from sklearn.neighbors import KDTree;\n",
    "Z = model.wv.syn0\n",
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    return kmeans_clustering.cluster_centers_, idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, clusters = clustering_on_wordvecs(Z, 100);\n",
    "centroid_map = dict(zip(model.wv.index2word, clusters));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt;\n",
    "from itertools import cycle;\n",
    "\n",
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "#Closest points for each Cluster center is used to query the closest 20 points to it.\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "#Word Index is queried for each position in the above array, and added to a Dictionary.\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "#A DataFrame is generated from the dictionary.\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "top_words = get_top_words(model.wv.index2word, 5000, centers, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D://Scriptie//Data//models\")\n",
    "list_mod = glob.glob('*.w2v')\n",
    "\n",
    "for i in list(range(1815,1906,10)):\n",
    "    mfn = [m for m in list_mod if int(m[0:4]) == i]\n",
    "    model = Word2Vec.load(mfn[0])\n",
    "    print(str(i) + \": \" + \" | \".join(list(dict(model.most_similar(\"vreemdeling\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

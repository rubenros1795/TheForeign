{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "import string\n",
    "import numpy\n",
    "import re\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "stop = [word.split() for word in open(\"C:\\\\Users\\\\Ruben\\\\Documents\\\\Artikelen\\\\Joris\\\\stopwords-nl.txt\", 'r', encoding = \"utf-8\").readlines()]\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname, encoding = 'utf-8') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "all_txt = glob.glob('*.txt')\n",
    "\n",
    "def Txt_Lines(txt):\n",
    "    with open(txt, encoding = 'utf-8') as f:\n",
    "            content = f.read().split('\\n')\n",
    "    \n",
    "    lines = list()\n",
    "    \n",
    "    for line in content:\n",
    "        tmp = line.split(' ')\n",
    "        lines.append(tmp)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Sentences(start_year, period, lim):\n",
    "    os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "    all_txt = glob.glob('*.txt')\n",
    "    all_txt = [f for f in all_txt if f[6:12] == 'leaned']\n",
    "    list_txt_range = [t for t in all_txt if int(t[0:4]) >= start_year and int(t[0:4]) < start_year + period]\n",
    "    \n",
    "    sentences = list()\n",
    "    \n",
    "    # Get Size of Range\n",
    "    range_size = 0\n",
    "    numb_lines = 0\n",
    "    \n",
    "    for txt in list_txt_range:\n",
    "        statinfo = os.stat(txt)\n",
    "        range_size += int(statinfo.st_size)\n",
    "        numb_lines += file_len(txt)\n",
    "    \n",
    "    range_size = round(range_size/ 1000000)\n",
    "    sample_size = lim + 1\n",
    "    \n",
    "    if range_size > lim:\n",
    "        tot_lines_samp = round(sample_size * numb_lines / range_size)\n",
    "        print('range size = ' + str(range_size) + '. Range is too long, sample is taken')\n",
    "                \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.read().splitlines()\n",
    "                \n",
    "                if len(content) <= round(tot_lines_samp / 10):\n",
    "                    for line in content:\n",
    "                        sentences.append(line)\n",
    "                \n",
    "                if len(content) > round(tot_lines_samp / 10):\n",
    "                    content = random.sample(content, round(tot_lines_samp / 10))\n",
    "                    for line in content:\n",
    "                        sentences.append(line)\n",
    "    \n",
    "    else:\n",
    "        print('range size = ' + str(range_size) + '. Range falls within size limit. No sample necessary')        \n",
    "        for txt in list_txt_range:\n",
    "            with open(txt, encoding = 'utf-8') as f:\n",
    "                content = f.read().splitlines()\n",
    "                print(str(len(content)) + ' lines opened from: ' + txt)\n",
    "                for line in content:\n",
    "                        sentences.append(line)\n",
    "    \n",
    "    tmp = [line.split(' ') for line in sentences]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range size = 282. Range falls within size limit. No sample necessary\n",
      "22988 lines opened from: 1815_cleaned.txt\n",
      "22024 lines opened from: 1816_cleaned.txt\n",
      "23146 lines opened from: 1817_cleaned.txt\n",
      "24486 lines opened from: 1818_cleaned.txt\n",
      "20386 lines opened from: 1819_cleaned.txt\n",
      "23180 lines opened from: 1820_cleaned.txt\n",
      "23722 lines opened from: 1821_cleaned.txt\n",
      "22950 lines opened from: 1822_cleaned.txt\n",
      "23532 lines opened from: 1823_cleaned.txt\n",
      "24242 lines opened from: 1824_cleaned.txt\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 467. Range falls within size limit. No sample necessary\n",
      "23510 lines opened from: 1825_cleaned.txt\n",
      "24066 lines opened from: 1826_cleaned.txt\n",
      "22404 lines opened from: 1827_cleaned.txt\n",
      "23650 lines opened from: 1828_cleaned.txt\n",
      "25424 lines opened from: 1829_cleaned.txt\n",
      "28104 lines opened from: 1830_cleaned.txt\n",
      "31080 lines opened from: 1831_cleaned.txt\n",
      "35238 lines opened from: 1832_cleaned.txt\n",
      "33646 lines opened from: 1833_cleaned.txt\n",
      "37462 lines opened from: 1834_cleaned.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n",
      "range size = 644. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1213. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1721. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1961. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2559. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 3419. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 5112. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 6834. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 10 YP\n",
    "\n",
    "for count, year in enumerate(list(range(1815,1915,10))):\n",
    "    \n",
    "    if count == 0:\n",
    "        sentences = Load_Sentences(year, 10, 550)\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=180, workers=6, iter=4, size = 100, window = 12)\n",
    "        os.chdir(\"D://Scriptie//Data//models//10yp_s100\")\n",
    "        model.wv.save_word2vec_format(str(year)+\"_s100_10yp_model.bin\", binary=True)\n",
    "\n",
    "        #model.save(str(year)+\"_w120_10yp_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "        \n",
    "    if count != 0:\n",
    "        sentences = Load_Sentences(year, 10, 550)\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models//10yp_s100\")\n",
    "\n",
    "        model.wv.save_word2vec_format(str(year)+\"_s100_10yp_model.bin\", binary=True)\n",
    "\n",
    "        #model.save(str(year)+\"_w175_10yp_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range size = 352. Range falls within size limit. No sample necessary\n",
      "37446 lines opened from: 1840_cleaned.txt\n",
      "36674 lines opened from: 1841_cleaned.txt\n",
      "37856 lines opened from: 1842_cleaned.txt\n",
      "36684 lines opened from: 1843_cleaned.txt\n",
      "50990 lines opened from: 1844_cleaned.txt\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 352. Range falls within size limit. No sample necessary\n",
      "37446 lines opened from: 1840_cleaned.txt\n",
      "36674 lines opened from: 1841_cleaned.txt\n",
      "37856 lines opened from: 1842_cleaned.txt\n",
      "36684 lines opened from: 1843_cleaned.txt\n",
      "50990 lines opened from: 1844_cleaned.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n",
      "range size = 569. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 643. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 765. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 956. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1057. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 904. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1107. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1451. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1541. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1878. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2435. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2677. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2801. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 4033. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in list(range(1840,1915,5)):\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "        sentences = Load_Sentences_5(i)\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=180, workers=6, iter=5, window = 200)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(i)+\"_5yp_model.w2v\")\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "\n",
    "        \n",
    "    if count > 0:\n",
    "        sentences = Load_Sentences_5(i)\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models\")\n",
    "        model.save(str(i)+\"_5yp_model.w2v\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "        print(\"Model saved, on to the next\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range size = 298. Range falls within size limit. No sample necessary\n",
      "28104 lines opened from: 1830_cleaned.txt\n",
      "31080 lines opened from: 1831_cleaned.txt\n",
      "35238 lines opened from: 1832_cleaned.txt\n",
      "33646 lines opened from: 1833_cleaned.txt\n",
      "37462 lines opened from: 1834_cleaned.txt\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 293. Range falls within size limit. No sample necessary\n",
      "35132 lines opened from: 1835_cleaned.txt\n",
      "35134 lines opened from: 1836_cleaned.txt\n",
      "33600 lines opened from: 1837_cleaned.txt\n",
      "34552 lines opened from: 1838_cleaned.txt\n",
      "36196 lines opened from: 1839_cleaned.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n",
      "range size = 352. Range falls within size limit. No sample necessary\n",
      "37446 lines opened from: 1840_cleaned.txt\n",
      "36674 lines opened from: 1841_cleaned.txt\n",
      "37856 lines opened from: 1842_cleaned.txt\n",
      "36684 lines opened from: 1843_cleaned.txt\n",
      "50990 lines opened from: 1844_cleaned.txt\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 569. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 643. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 765. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 956. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1057. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 904. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1107. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1451. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1541. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1878. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2435. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2677. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2801. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 4033. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 5 YP\n",
    "\n",
    "for count, year in enumerate(list(range(1830,1911,5))):\n",
    "    \n",
    "    if count == 0:\n",
    "        sentences = Load_Sentences(year, 5, 550)\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=180, workers=6, iter=4, size = 100, window = 12)\n",
    "        os.chdir(\"D://Scriptie//Data//models//5yp_s100\")\n",
    "        model.wv.save_word2vec_format(str(year)+\"_s100_5yp_model.bin\", binary=True)\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "        \n",
    "    if count != 0:\n",
    "        sentences = Load_Sentences(year, 5, 550)\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models//5yp_s100\")\n",
    "        model.wv.save_word2vec_format(str(year)+\"_s100_5yp_model.bin\", binary=True)\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range size = 298. Range falls within size limit. No sample necessary\n",
      "28104 lines opened from: 1830_cleaned.txt\n",
      "31080 lines opened from: 1831_cleaned.txt\n",
      "35238 lines opened from: 1832_cleaned.txt\n",
      "33646 lines opened from: 1833_cleaned.txt\n",
      "37462 lines opened from: 1834_cleaned.txt\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 293. Range falls within size limit. No sample necessary\n",
      "35132 lines opened from: 1835_cleaned.txt\n",
      "35134 lines opened from: 1836_cleaned.txt\n",
      "33600 lines opened from: 1837_cleaned.txt\n",
      "34552 lines opened from: 1838_cleaned.txt\n",
      "36196 lines opened from: 1839_cleaned.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, on to the next\n",
      "\n",
      "range size = 352. Range falls within size limit. No sample necessary\n",
      "37446 lines opened from: 1840_cleaned.txt\n",
      "36674 lines opened from: 1841_cleaned.txt\n",
      "37856 lines opened from: 1842_cleaned.txt\n",
      "36684 lines opened from: 1843_cleaned.txt\n",
      "50990 lines opened from: 1844_cleaned.txt\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 569. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 643. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 765. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 956. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1057. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 904. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1107. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1451. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1541. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 1878. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2435. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2677. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 2801. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 4033. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 5 YP\n",
    "\n",
    "for count, year in enumerate(list(range(1830,1911,5))):\n",
    "    \n",
    "    if count == 0:\n",
    "        sentences = Load_Sentences(year, 5, 550)\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=180, workers=6, iter=4, size = 100, window = 12)\n",
    "        os.chdir(\"D://Scriptie//Data//models//5yp_s120\")\n",
    "        model.wv.save_word2vec_format(str(year)+\"_s120_5yp_model.bin\", binary=True)\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "        \n",
    "    if count != 0:\n",
    "        sentences = Load_Sentences(year, 5, 550)\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models//5yp_s120\")\n",
    "        model.wv.save_word2vec_format(str(year)+\"_s120_5yp_model.bin\", binary=True)\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range size = 1393. Range is too long, sample is taken\n",
      "Model saved, on to the next\n",
      "\n",
      "range size = 5433. Range is too long, sample is taken\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e96f20bd03a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoad_Sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m650\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-756ff698a15e>\u001b[0m in \u001b[0;36mLoad_Sentences\u001b[1;34m(start_year, period, lim)\u001b[0m\n\u001b[0;32m     45\u001b[0m                         \u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-756ff698a15e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m                         \u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Periods\n",
    "\n",
    "for count, year in enumerate(list(range(1815,1915,35))):\n",
    "    \n",
    "    if count == 0:\n",
    "        sentences = Load_Sentences(year, 30, 650)\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=180, workers=6, iter=4, size = 100, window = 12)\n",
    "        os.chdir(\"D://Scriptie//Data//models//periods_s150\")\n",
    "        model.wv.save_word2vec_format(str(year)+\"_model.bin\", binary=True)\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")\n",
    "        \n",
    "    if count != 0:\n",
    "        sentences = Load_Sentences(year, 30, 650)\n",
    "        model.build_vocab(sentences, update=True)\n",
    "        model.train(sentences, total_examples = model.corpus_count, start_alpha = model.alpha, end_alpha = model.min_alpha, epochs = model.iter)\n",
    "        os.chdir(\"D://Scriptie//Data//models//periods_s150\")\n",
    "        model.wv.save_word2vec_format(str(year)+\"_model.bin\", binary=True)\n",
    "        print(\"Model saved, on to the next\\n\")\n",
    "        os.chdir(\"D://Scriptie//Data//lines//cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('vreerade', 0.8855116963386536),\n",
       " ('bevriende', 0.7039967179298401),\n",
       " ('europesche', 0.6816353797912598),\n",
       " ('europefche', 0.6473175287246704),\n",
       " ('europesehe', 0.6139569282531738),\n",
       " ('engelsche', 0.5938653945922852),\n",
       " ('neutrale', 0.5856097936630249),\n",
       " ('andere', 0.5819052457809448),\n",
       " ('europische', 0.5807040333747864),\n",
       " ('onzijdige', 0.5648606419563293),\n",
       " ('britsche', 0.564644992351532),\n",
       " ('enropesche', 0.5566414594650269),\n",
       " ('beschermende', 0.5556291937828064),\n",
       " ('engeische', 0.5508261322975159),\n",
       " ('britfche', 0.5486218333244324),\n",
       " ('hollandsche', 0.5469624400138855),\n",
       " ('inlandsche', 0.5459616184234619),\n",
       " ('frankische', 0.5446786284446716),\n",
       " ('europeesche', 0.5395474433898926),\n",
       " ('geallieerde', 0.5383212566375732)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('vreemde', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category_id'] = df['Product'].factorize()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
